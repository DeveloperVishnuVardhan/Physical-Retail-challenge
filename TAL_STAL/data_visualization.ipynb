{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal of this notebook is to find if we can extract the following information from our dataset.\n",
    "\n",
    "1. For each Associated action do we have the action labels.\n",
    "2. If we have Associated action labels for each frame do we have information about the start and end frames of each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_path = \"/Users/jyothivishnuvardhankolla/Desktop/CS-7180 Advanced Perception/Physical_Retail_Challenge/Data/activity_based/anns\"\n",
    "anns_path = \"/Users/jyothivishnuvardhankolla/Desktop/CS-7180 Advanced Perception/Physical_Retail_Challenge/Data/activity_based/anns\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is given in readme that each frame has an unique id so lets check the total number of frames and total number of unique id's present**\n",
    "\n",
    "**Why do this?**\n",
    "if the number of unique id's are less than the number of frames it means we can extract information about start and end frames for a particular action in the untrimmed video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique id's 4726 and total number of files is 73683\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_unique_ids(anns_paths):\n",
    "    unique_ids = set()\n",
    "\n",
    "    for file in os.listdir(anns_paths):\n",
    "        if file.endswith('.txt'):\n",
    "            file_path = os.path.join(anns_path, file)\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    data = line.strip().split(',')\n",
    "                    unique_id = data[0].strip('[]')\n",
    "                    unique_ids.add(unique_id)\n",
    "\n",
    "    return len(unique_ids)\n",
    "\n",
    "print(f\"Total number of unique id's {count_unique_ids(anns_path)} and total number of files is {len(os.listdir(anns_path))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now as we have found The number of unique_id's is very less than the total number of files now let's prepare a input txt file for the TAL problem.**\n",
    "\n",
    "**Data Format:**\n",
    "1. For each unique action in the video sequence temporally assign a data point as (start_frame, end_frame, unique_id, class)\n",
    "2. Visualize each trimmed video to see if our logic was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_results(final_annotations_list, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        for data in final_annotations_list:\n",
    "            line = f\"{data[0]}, {data[1]}, {data[2]}, {data[3]}\\n\"\n",
    "            file.write(line)\n",
    "\n",
    "\n",
    "def create_TAL_annotations(anns_path: str):\n",
    "    final_annotations = []\n",
    "    skipped_frames = []\n",
    "    last_unique_id, last_class, start_file, prev_file = None, None, None, None\n",
    "\n",
    "    # Sort the files to ensure they are sorted in order.\n",
    "    files = sorted([f for f in os.listdir(anns_path) if f.endswith('.txt')])\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(anns_path, file)\n",
    "        with open(file_path, 'r') as f:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                skipped_frames.append(file)\n",
    "                continue\n",
    "            data = line.strip().split(',')\n",
    "            unique_id = data[0].strip('[]')\n",
    "            action_class = int(data[1].strip())\n",
    "\n",
    "        if last_unique_id is None:\n",
    "            last_unique_id, last_class, start_file = unique_id, action_class, file\n",
    "        elif unique_id != last_unique_id or action_class != last_class:\n",
    "            # change detected record the clip\n",
    "            final_annotations.append((start_file, prev_file, last_unique_id, last_class))\n",
    "            last_unique_id, last_class, start_file = unique_id, action_class, file\n",
    "\n",
    "        prev_file = file\n",
    "\n",
    "    # Adding the last segment if it exists\n",
    "    if last_unique_id is not None:\n",
    "        final_annotations.append((start_file, prev_file, last_unique_id, last_class))\n",
    "\n",
    "    return final_annotations\n",
    "          \n",
    "\n",
    "\n",
    "final_anns_list = create_TAL_annotations(anns_path=anns_path)\n",
    "\n",
    "# save the final_annotations in a text file.\n",
    "output_file = \"/Users/jyothivishnuvardhankolla/Desktop/CS-7180 Advanced Perception/Physical_Retail_Challenge/Data/activity_based/final_anns.txt\"\n",
    "save_results(final_anns_list, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
